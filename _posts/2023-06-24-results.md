---
layout: post
title: results
---

# **Data Modeling and Analysis**
This includes findings from the mock poster on the conducted t-test as the statistic hypothesis test and the predictors created through machine learning.

# Statistic Hypothesis Test
For this, we'll be using the **time series hypothesis test** as described [here](https://elvyna.github.io/2018/time-series-hypothesis-testing/#:~:text=Time%20Series%20Hypothesis%20Test,have%20residuals%20of%20the%20series.).

The first thing we need to do is load our data set into the Python Notebook. Just for background, we have the tweets between the 25 December 2021 to 23 June 2023 which span the 45 days before the start of campaign period until 45 days after elections.

From this, we note the following important dates:
- Campaign period (for nationally elected positions): 8 Feb 2022 to 7 May 2022
- Election silence: 8 May 2022
- Election day (for non-absentee voters in the Philippines): 9 May 2022

```python
import pandas as pd

df = pd.read_csv('cs132_tweets_model_and_results.csv')
pd.set_option('display.max_colwidth', None)
df.head().style.set_properties(**{'text-align': 'left'})
```



<style type="text/css">
#T_10ee2_row0_col0, #T_10ee2_row0_col1, #T_10ee2_row0_col2, #T_10ee2_row0_col3, #T_10ee2_row0_col4, #T_10ee2_row0_col5, #T_10ee2_row0_col6, #T_10ee2_row0_col7, #T_10ee2_row0_col8, #T_10ee2_row0_col9, #T_10ee2_row1_col0, #T_10ee2_row1_col1, #T_10ee2_row1_col2, #T_10ee2_row1_col3, #T_10ee2_row1_col4, #T_10ee2_row1_col5, #T_10ee2_row1_col6, #T_10ee2_row1_col7, #T_10ee2_row1_col8, #T_10ee2_row1_col9, #T_10ee2_row2_col0, #T_10ee2_row2_col1, #T_10ee2_row2_col2, #T_10ee2_row2_col3, #T_10ee2_row2_col4, #T_10ee2_row2_col5, #T_10ee2_row2_col6, #T_10ee2_row2_col7, #T_10ee2_row2_col8, #T_10ee2_row2_col9, #T_10ee2_row3_col0, #T_10ee2_row3_col1, #T_10ee2_row3_col2, #T_10ee2_row3_col3, #T_10ee2_row3_col4, #T_10ee2_row3_col5, #T_10ee2_row3_col6, #T_10ee2_row3_col7, #T_10ee2_row3_col8, #T_10ee2_row3_col9, #T_10ee2_row4_col0, #T_10ee2_row4_col1, #T_10ee2_row4_col2, #T_10ee2_row4_col3, #T_10ee2_row4_col4, #T_10ee2_row4_col5, #T_10ee2_row4_col6, #T_10ee2_row4_col7, #T_10ee2_row4_col8, #T_10ee2_row4_col9 {
  text-align: left;
}
</style>
<table id="T_10ee2">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_10ee2_level0_col0" class="col_heading level0 col0" >ID</th>
      <th id="T_10ee2_level0_col1" class="col_heading level0 col1" >Account type
"Identified
Anonymous
Media"</th>
      <th id="T_10ee2_level0_col2" class="col_heading level0 col2" >Tweet Type</th>
      <th id="T_10ee2_level0_col3" class="col_heading level0 col3" >Tweet Type (2)</th>
      <th id="T_10ee2_level0_col4" class="col_heading level0 col4" >Tweet Type (3)</th>
      <th id="T_10ee2_level0_col5" class="col_heading level0 col5" >Date posted</th>
      <th id="T_10ee2_level0_col6" class="col_heading level0 col6" >Screenshot</th>
      <th id="T_10ee2_level0_col7" class="col_heading level0 col7" >Tweet ID</th>
      <th id="T_10ee2_level0_col8" class="col_heading level0 col8" >Tweet</th>
      <th id="T_10ee2_level0_col9" class="col_heading level0 col9" >Content type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_10ee2_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_10ee2_row0_col0" class="data row0 col0" >00-1</td>
      <td id="T_10ee2_row0_col1" class="data row0 col1" >Identified</td>
      <td id="T_10ee2_row0_col2" class="data row0 col2" >Reply</td>
      <td id="T_10ee2_row0_col3" class="data row0 col3" >nan</td>
      <td id="T_10ee2_row0_col4" class="data row0 col4" >nan</td>
      <td id="T_10ee2_row0_col5" class="data row0 col5" >12/26/21 22:35</td>
      <td id="T_10ee2_row0_col6" class="data row0 col6" >00-1.png</td>
      <td id="T_10ee2_row0_col7" class="data row0 col7" >1475113095395966976</td>
      <td id="T_10ee2_row0_col8" class="data row0 col8" >@_ultravioletred @jersonality If their ED was done by a CPP cadre, the educator would've been reprimanded for sure. Whoever that educator is, they're probably still thinking in the 90s. Wake up kas, RA/RJ split is not that relevant anymore, lol. Embrace your comrades like what Neri, Labog, and Zarate did.</td>
      <td id="T_10ee2_row0_col9" class="data row0 col9" >Rational</td>
    </tr>
    <tr>
      <th id="T_10ee2_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_10ee2_row1_col0" class="data row1 col0" >00-2</td>
      <td id="T_10ee2_row1_col1" class="data row1 col1" >Identified</td>
      <td id="T_10ee2_row1_col2" class="data row1 col2" >Reply</td>
      <td id="T_10ee2_row1_col3" class="data row1 col3" >nan</td>
      <td id="T_10ee2_row1_col4" class="data row1 col4" >nan</td>
      <td id="T_10ee2_row1_col5" class="data row1 col5" >12/28/21 13:25</td>
      <td id="T_10ee2_row1_col6" class="data row1 col6" >00-2.png</td>
      <td id="T_10ee2_row1_col7" class="data row1 col7" >1475699422982189056</td>
      <td id="T_10ee2_row1_col8" class="data row1 col8" >@lukatmhe Colmenarez family ay part sa legal front ng NPA. At saka may first cousin si Angel Locsin na big time drug dealer ng shabu sa Cebu. Siya ay former Local Beauty Queen. Mismo si Neri Colmenarez naki usap sa mga police na wag galawin or itumba.</td>
      <td id="T_10ee2_row1_col9" class="data row1 col9" >Rational</td>
    </tr>
    <tr>
      <th id="T_10ee2_level0_row2" class="row_heading level0 row2" >2</th>
      <td id="T_10ee2_row2_col0" class="data row2 col0" >00-3</td>
      <td id="T_10ee2_row2_col1" class="data row2 col1" >Anonymous</td>
      <td id="T_10ee2_row2_col2" class="data row2 col2" >Reply</td>
      <td id="T_10ee2_row2_col3" class="data row2 col3" >nan</td>
      <td id="T_10ee2_row2_col4" class="data row2 col4" >nan</td>
      <td id="T_10ee2_row2_col5" class="data row2 col5" >12/29/21 20:47</td>
      <td id="T_10ee2_row2_col6" class="data row2 col6" >00-3.png</td>
      <td id="T_10ee2_row2_col7" class="data row2 col7" >1476173017458016256</td>
      <td id="T_10ee2_row2_col8" class="data row2 col8" >@weirdnow1 @pnagovph yay ðŸ˜‚ðŸ˜‚ðŸ˜‚ kaya pa nmn mag pakamatay yang sina Elago,Zarate,Colmenares,Nato Reyes sa pakiki bakbakan.Tanggaaa yung mga nasapi sa NPA</td>
      <td id="T_10ee2_row2_col9" class="data row2 col9" >Emotional</td>
    </tr>
    <tr>
      <th id="T_10ee2_level0_row3" class="row_heading level0 row3" >3</th>
      <td id="T_10ee2_row3_col0" class="data row3 col0" >00-4</td>
      <td id="T_10ee2_row3_col1" class="data row3 col1" >Anonymous</td>
      <td id="T_10ee2_row3_col2" class="data row3 col2" >Reply</td>
      <td id="T_10ee2_row3_col3" class="data row3 col3" >nan</td>
      <td id="T_10ee2_row3_col4" class="data row3 col4" >nan</td>
      <td id="T_10ee2_row3_col5" class="data row3 col5" >01/07/22 14:37</td>
      <td id="T_10ee2_row3_col6" class="data row3 col6" >00-4.png</td>
      <td id="T_10ee2_row3_col7" class="data row3 col7" >1479341371219902464</td>
      <td id="T_10ee2_row3_col8" class="data row3 col8" >@ColmenaresPH Shut Up Colmenares NPA. SALOT KA

 #Nerveagain</td>
      <td id="T_10ee2_row3_col9" class="data row3 col9" >Emotional</td>
    </tr>
    <tr>
      <th id="T_10ee2_level0_row4" class="row_heading level0 row4" >4</th>
      <td id="T_10ee2_row4_col0" class="data row4 col0" >00-5</td>
      <td id="T_10ee2_row4_col1" class="data row4 col1" >Anonymous</td>
      <td id="T_10ee2_row4_col2" class="data row4 col2" >Reply</td>
      <td id="T_10ee2_row4_col3" class="data row4 col3" >nan</td>
      <td id="T_10ee2_row4_col4" class="data row4 col4" >nan</td>
      <td id="T_10ee2_row4_col5" class="data row4 col5" >01/08/22 06:25</td>
      <td id="T_10ee2_row4_col6" class="data row4 col6" >00-5.png</td>
      <td id="T_10ee2_row4_col7" class="data row4 col7" >1479580070922846208</td>
      <td id="T_10ee2_row4_col8" class="data row4 col8" >AHAHAHAHA neri niyo NPA</td>
      <td id="T_10ee2_row4_col9" class="data row4 col9" >Emotional</td>
    </tr>
  </tbody>
</table>



## Creating a daily tally of tweets
Here, we create a complete log of the included dates and the corresponding frequency of tweets for each day.

```python
# Copy the dataframe to one we can process
df_daily = df.copy()

# Transform the data to reflect daily logs
df_daily['Date posted'] = pd.to_datetime(df['Date posted'])
df_daily['Date posted'] = df_daily['Date posted'].dt.strftime('%Y-%m-%d')
df_daily['Date'] = pd.to_datetime(df_daily['Date posted'], format='%Y-%m-%d')

pd.set_option('display.max_colwidth', None)
df_daily.head().style.set_properties(**{'text-align': 'left'})
```



<style type="text/css">
#T_35cc7_row0_col0, #T_35cc7_row0_col1, #T_35cc7_row0_col2, #T_35cc7_row0_col3, #T_35cc7_row0_col4, #T_35cc7_row0_col5, #T_35cc7_row0_col6, #T_35cc7_row0_col7, #T_35cc7_row0_col8, #T_35cc7_row0_col9, #T_35cc7_row0_col10, #T_35cc7_row1_col0, #T_35cc7_row1_col1, #T_35cc7_row1_col2, #T_35cc7_row1_col3, #T_35cc7_row1_col4, #T_35cc7_row1_col5, #T_35cc7_row1_col6, #T_35cc7_row1_col7, #T_35cc7_row1_col8, #T_35cc7_row1_col9, #T_35cc7_row1_col10, #T_35cc7_row2_col0, #T_35cc7_row2_col1, #T_35cc7_row2_col2, #T_35cc7_row2_col3, #T_35cc7_row2_col4, #T_35cc7_row2_col5, #T_35cc7_row2_col6, #T_35cc7_row2_col7, #T_35cc7_row2_col8, #T_35cc7_row2_col9, #T_35cc7_row2_col10, #T_35cc7_row3_col0, #T_35cc7_row3_col1, #T_35cc7_row3_col2, #T_35cc7_row3_col3, #T_35cc7_row3_col4, #T_35cc7_row3_col5, #T_35cc7_row3_col6, #T_35cc7_row3_col7, #T_35cc7_row3_col8, #T_35cc7_row3_col9, #T_35cc7_row3_col10, #T_35cc7_row4_col0, #T_35cc7_row4_col1, #T_35cc7_row4_col2, #T_35cc7_row4_col3, #T_35cc7_row4_col4, #T_35cc7_row4_col5, #T_35cc7_row4_col6, #T_35cc7_row4_col7, #T_35cc7_row4_col8, #T_35cc7_row4_col9, #T_35cc7_row4_col10 {
  text-align: left;
}
</style>
<table id="T_35cc7">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_35cc7_level0_col0" class="col_heading level0 col0" >ID</th>
      <th id="T_35cc7_level0_col1" class="col_heading level0 col1" >Account type
"Identified
Anonymous
Media"</th>
      <th id="T_35cc7_level0_col2" class="col_heading level0 col2" >Tweet Type</th>
      <th id="T_35cc7_level0_col3" class="col_heading level0 col3" >Tweet Type (2)</th>
      <th id="T_35cc7_level0_col4" class="col_heading level0 col4" >Tweet Type (3)</th>
      <th id="T_35cc7_level0_col5" class="col_heading level0 col5" >Date posted</th>
      <th id="T_35cc7_level0_col6" class="col_heading level0 col6" >Screenshot</th>
      <th id="T_35cc7_level0_col7" class="col_heading level0 col7" >Tweet ID</th>
      <th id="T_35cc7_level0_col8" class="col_heading level0 col8" >Tweet</th>
      <th id="T_35cc7_level0_col9" class="col_heading level0 col9" >Content type</th>
      <th id="T_35cc7_level0_col10" class="col_heading level0 col10" >Date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_35cc7_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_35cc7_row0_col0" class="data row0 col0" >00-1</td>
      <td id="T_35cc7_row0_col1" class="data row0 col1" >Identified</td>
      <td id="T_35cc7_row0_col2" class="data row0 col2" >Reply</td>
      <td id="T_35cc7_row0_col3" class="data row0 col3" >nan</td>
      <td id="T_35cc7_row0_col4" class="data row0 col4" >nan</td>
      <td id="T_35cc7_row0_col5" class="data row0 col5" >2021-12-26</td>
      <td id="T_35cc7_row0_col6" class="data row0 col6" >00-1.png</td>
      <td id="T_35cc7_row0_col7" class="data row0 col7" >1475113095395966976</td>
      <td id="T_35cc7_row0_col8" class="data row0 col8" >@_ultravioletred @jersonality If their ED was done by a CPP cadre, the educator would've been reprimanded for sure. Whoever that educator is, they're probably still thinking in the 90s. Wake up kas, RA/RJ split is not that relevant anymore, lol. Embrace your comrades like what Neri, Labog, and Zarate did.</td>
      <td id="T_35cc7_row0_col9" class="data row0 col9" >Rational</td>
      <td id="T_35cc7_row0_col10" class="data row0 col10" >2021-12-26 00:00:00</td>
    </tr>
    <tr>
      <th id="T_35cc7_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_35cc7_row1_col0" class="data row1 col0" >00-2</td>
      <td id="T_35cc7_row1_col1" class="data row1 col1" >Identified</td>
      <td id="T_35cc7_row1_col2" class="data row1 col2" >Reply</td>
      <td id="T_35cc7_row1_col3" class="data row1 col3" >nan</td>
      <td id="T_35cc7_row1_col4" class="data row1 col4" >nan</td>
      <td id="T_35cc7_row1_col5" class="data row1 col5" >2021-12-28</td>
      <td id="T_35cc7_row1_col6" class="data row1 col6" >00-2.png</td>
      <td id="T_35cc7_row1_col7" class="data row1 col7" >1475699422982189056</td>
      <td id="T_35cc7_row1_col8" class="data row1 col8" >@lukatmhe Colmenarez family ay part sa legal front ng NPA. At saka may first cousin si Angel Locsin na big time drug dealer ng shabu sa Cebu. Siya ay former Local Beauty Queen. Mismo si Neri Colmenarez naki usap sa mga police na wag galawin or itumba.</td>
      <td id="T_35cc7_row1_col9" class="data row1 col9" >Rational</td>
      <td id="T_35cc7_row1_col10" class="data row1 col10" >2021-12-28 00:00:00</td>
    </tr>
    <tr>
      <th id="T_35cc7_level0_row2" class="row_heading level0 row2" >2</th>
      <td id="T_35cc7_row2_col0" class="data row2 col0" >00-3</td>
      <td id="T_35cc7_row2_col1" class="data row2 col1" >Anonymous</td>
      <td id="T_35cc7_row2_col2" class="data row2 col2" >Reply</td>
      <td id="T_35cc7_row2_col3" class="data row2 col3" >nan</td>
      <td id="T_35cc7_row2_col4" class="data row2 col4" >nan</td>
      <td id="T_35cc7_row2_col5" class="data row2 col5" >2021-12-29</td>
      <td id="T_35cc7_row2_col6" class="data row2 col6" >00-3.png</td>
      <td id="T_35cc7_row2_col7" class="data row2 col7" >1476173017458016256</td>
      <td id="T_35cc7_row2_col8" class="data row2 col8" >@weirdnow1 @pnagovph yay ðŸ˜‚ðŸ˜‚ðŸ˜‚ kaya pa nmn mag pakamatay yang sina Elago,Zarate,Colmenares,Nato Reyes sa pakiki bakbakan.Tanggaaa yung mga nasapi sa NPA</td>
      <td id="T_35cc7_row2_col9" class="data row2 col9" >Emotional</td>
      <td id="T_35cc7_row2_col10" class="data row2 col10" >2021-12-29 00:00:00</td>
    </tr>
    <tr>
      <th id="T_35cc7_level0_row3" class="row_heading level0 row3" >3</th>
      <td id="T_35cc7_row3_col0" class="data row3 col0" >00-4</td>
      <td id="T_35cc7_row3_col1" class="data row3 col1" >Anonymous</td>
      <td id="T_35cc7_row3_col2" class="data row3 col2" >Reply</td>
      <td id="T_35cc7_row3_col3" class="data row3 col3" >nan</td>
      <td id="T_35cc7_row3_col4" class="data row3 col4" >nan</td>
      <td id="T_35cc7_row3_col5" class="data row3 col5" >2022-01-07</td>
      <td id="T_35cc7_row3_col6" class="data row3 col6" >00-4.png</td>
      <td id="T_35cc7_row3_col7" class="data row3 col7" >1479341371219902464</td>
      <td id="T_35cc7_row3_col8" class="data row3 col8" >@ColmenaresPH Shut Up Colmenares NPA. SALOT KA

 #Nerveagain</td>
      <td id="T_35cc7_row3_col9" class="data row3 col9" >Emotional</td>
      <td id="T_35cc7_row3_col10" class="data row3 col10" >2022-01-07 00:00:00</td>
    </tr>
    <tr>
      <th id="T_35cc7_level0_row4" class="row_heading level0 row4" >4</th>
      <td id="T_35cc7_row4_col0" class="data row4 col0" >00-5</td>
      <td id="T_35cc7_row4_col1" class="data row4 col1" >Anonymous</td>
      <td id="T_35cc7_row4_col2" class="data row4 col2" >Reply</td>
      <td id="T_35cc7_row4_col3" class="data row4 col3" >nan</td>
      <td id="T_35cc7_row4_col4" class="data row4 col4" >nan</td>
      <td id="T_35cc7_row4_col5" class="data row4 col5" >2022-01-08</td>
      <td id="T_35cc7_row4_col6" class="data row4 col6" >00-5.png</td>
      <td id="T_35cc7_row4_col7" class="data row4 col7" >1479580070922846208</td>
      <td id="T_35cc7_row4_col8" class="data row4 col8" >AHAHAHAHA neri niyo NPA</td>
      <td id="T_35cc7_row4_col9" class="data row4 col9" >Emotional</td>
      <td id="T_35cc7_row4_col10" class="data row4 col10" >2022-01-08 00:00:00</td>
    </tr>
  </tbody>
</table>



```python
# Create a counts dataframe
s_freqinc = df_daily['Date'].value_counts()
df_freqinc = s_freqinc.to_frame()
df_freqinc.columns = ['Frequency']
df_freqinc.head()

# Create range of dates
new_dates = pd.date_range(start='2021-12-25',end='2022-06-23',freq='D')

# Reindex the dates
df_freq = df_freqinc.reindex(new_dates).fillna(0)
df_freq['Frequency'] = df_freq['Frequency'].astype('int')
df_freq.head()
```



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2021-12-25</th>
      <td>0</td>
    </tr>
    <tr>
      <th>2021-12-26</th>
      <td>1</td>
    </tr>
    <tr>
      <th>2021-12-27</th>
      <td>0</td>
    </tr>
    <tr>
      <th>2021-12-28</th>
      <td>1</td>
    </tr>
    <tr>
      <th>2021-12-29</th>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>


```python
df_datefreq = df_freq.copy()
df_datefreq['Date'] = df_datefreq.index
df_datefreq.reset_index(drop=True, inplace=True)
df_datefreq = df_datefreq[['Date', 'Frequency']]

df_freq.to_csv('cs132_datefreq.csv')
df_datefreq.head()
```



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Date</th>
      <th>Frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2021-12-25</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2021-12-26</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2021-12-27</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2021-12-28</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2021-12-29</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>


## Testing for stationarity

One of the diagnoses done in time series analyses is to test for stationarity, or whether the mean and variance of the time series is constant over time. This is important because many time series models and statistical tests assume that the time series is stationary.

A way to test for stationarity is through autocorrelation, in which we compare a time series to a copy of itself shifted backward in time ("lagged") to see whether past data is a good predictor of future data. We can do this by using the `autocorrelation_plot` function from the `pandas.plotting` module.

In the plot, a $y$-value of...

- $1$: means that the time series is perfectly correlated with itself at that lag (i.e., past data is a perfect predictor of future data),
- $0$: means that the time series is not correlated with itself at that lag (i.e., past data is not a good predictor of future data),
- $-1$ means that the time series is perfectly anti-correlated with itself at that lag (i.e., past data is a perfect inverse predictor of future data; large past data values is likely followed by smaller values).

```python
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import statsmodels.api as sm

# Change lags here
lags = 50

with mpl.rc_context():
    mpl.rc("figure", figsize=(14,8))
    sm.graphics.tsa.plot_acf(df_freq['Frequency'], lags=lags)
    plt.grid()
    plt.xticks(np.arange(0, lags+1, 1.0))
    plt.xlabel('Lag in days, $k$')
    plt.show()

# Store ACF values and confidence intervals
acf_random, acf_random_confidence = sm.tsa.stattools.acf(df_freq['Frequency'], nlags=lags, alpha=0.05)

# Print largest ACF value excluding lag 0
max_acf = max(acf_random[1:])
print(f'Largest ACF value: {max_acf} at lag k={np.argmax(acf_random[1:]) + 1}')
```

![png]({{ site.baseurl }}/public/images/results/results_8_0.png){: .center-image }

    Largest ACF value: 0.2710252373376295 at lag k=1

The blue region in the plot represents the confidence interval, and any autocorrelation values outside this region are considered statistically significant. We can see from the plot that the autocorrelation values are mostly within the confidence interval, except for lags $k=1$, $k=8$, $k=10$, and $k=11$.

## Test for normality

In order to determine whether we can use a parametric or non-parametric test, we need to check if the data is normally distributed. We do this by using the Shapiro-Wilk test.

H0: The data is normally distributed

H1: The data is not normally distributed

```python
from scipy.stats import shapiro

# Normality tests
stat, p = shapiro(df_freq['Frequency'])
print('[Shapiro-Wilk] statistic=%.3f, p=%.25f' % (stat, p))
if p < 0.05:
  print('[Shapiro-Wilk] Reject H0 (not normally distributed)')
else:
  print('[Shapiro-Wilk] Do not reject H0 (normally distributed)')
```

    [Shapiro-Wilk] statistic=0.761, p=0.0000000000000007134777684
    [Shapiro-Wilk] Reject H0 (not normally distributed)

## Mann-Whitney U test

Since the data was found to not be normally distributed, we have to use a non-parametric test. We use the Mann-Whitney U test to determine whether there is a significant difference between the averages of each period.

### Test 1: Before vs After Elections

H0: There is no difference between the frequency of tweets before and after elections.

H1: The frequency of tweets significantly change before and after elections.

- Election day (for non-absentee voters in the Philippines): 9 May 2022

```python
from scipy.stats import mannwhitneyu

# Create data groups
data_pre = df_datefreq[df_datefreq['Date'] < '2022-05-09']
data_post = df_datefreq[df_datefreq['Date'] > '2022-05-09']

# Perform Mann-Whitney U test
stat, p = mannwhitneyu(data_pre['Frequency'], data_post['Frequency'])
print('[Mann-Whitney] statistic=%.3f, p=%.25f' % (stat, p))
if p < 0.05:
  print('[Mann-Whitney] Reject H0 (significant difference)')
else:
  print('[Mann-Whitney] Do not reject H0 (no significant difference)')
```

    [Mann-Whitney] statistic=4322.000, p=0.0000090593996920068365882
    [Mann-Whitney] Reject H0 (significant difference)

We can see that $p<0.05$, leading us to the conclusion that **there is a significant difference between the average frequency of tweets before and after elections**.

### Test 2: During vs. Not During the Campaign Period
H0: There is no difference between the frequency of tweets included and excluded from the campaign period.

H1: The frequency of tweets significantly change included and excluded from the campaign period.

- Campaign period (for nationally elected positions): 8 Feb 2022 to 7 May 2022


```python
# Create data groups
mask = (df_datefreq['Date'] > '2022-02-08') & (df_datefreq['Date'] <= '2022-05-07')
data_inc = df_datefreq.loc[mask]
mask =  (df_datefreq['Date'] <= '2022-02-08') | (df_datefreq['Date'] > '2022-05-07')
data_ex = df_datefreq.loc[mask]

# Perform Mann-Whitney U test
stat, p = mannwhitneyu(data_inc['Frequency'], data_ex['Frequency'])
print('[Mann-Whitney] statistic=%.3f, p=%.25f' % (stat, p))
if p < 0.05:
  print('[Mann-Whitney] Reject H0 (significant difference)')
else:
  print('[Mann-Whitney] Do not reject H0 (no significant difference)')
```

    [Mann-Whitney] statistic=5185.500, p=0.0011691632314091570006565
    [Mann-Whitney] Reject H0 (significant difference)

We can see that $p<0.05$, leading us to the conclusion that **there is a significant difference between the average frequency of tweets during and not during the official campaign period**.

# Machine Learning
In this notebook, we follow instructions [here](https://towardsdatascience.com/the-complete-guide-to-time-series-forecasting-using-sklearn-pandas-and-numpy-7694c90e45c1) to try and predict the expected number of tweets in a day based on the previous day using Time Series Forecasting.

```python
plt.rcParams['font.size'] = 16

df = pd.read_csv('cs132_datefreq.csv')
df.columns = ['Date', 'Frequency']
pd.set_option('display.max_colwidth', None)
df.head().style.set_properties(**{'text-align': 'left'})
```



<style type="text/css">
#T_b8361_row0_col0, #T_b8361_row0_col1, #T_b8361_row1_col0, #T_b8361_row1_col1, #T_b8361_row2_col0, #T_b8361_row2_col1, #T_b8361_row3_col0, #T_b8361_row3_col1, #T_b8361_row4_col0, #T_b8361_row4_col1 {
  text-align: left;
}
</style>
<table id="T_b8361">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_b8361_level0_col0" class="col_heading level0 col0" >Date</th>
      <th id="T_b8361_level0_col1" class="col_heading level0 col1" >Frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_b8361_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_b8361_row0_col0" class="data row0 col0" >2021-12-25</td>
      <td id="T_b8361_row0_col1" class="data row0 col1" >0</td>
    </tr>
    <tr>
      <th id="T_b8361_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_b8361_row1_col0" class="data row1 col0" >2021-12-26</td>
      <td id="T_b8361_row1_col1" class="data row1 col1" >1</td>
    </tr>
    <tr>
      <th id="T_b8361_level0_row2" class="row_heading level0 row2" >2</th>
      <td id="T_b8361_row2_col0" class="data row2 col0" >2021-12-27</td>
      <td id="T_b8361_row2_col1" class="data row2 col1" >0</td>
    </tr>
    <tr>
      <th id="T_b8361_level0_row3" class="row_heading level0 row3" >3</th>
      <td id="T_b8361_row3_col0" class="data row3 col0" >2021-12-28</td>
      <td id="T_b8361_row3_col1" class="data row3 col1" >1</td>
    </tr>
    <tr>
      <th id="T_b8361_level0_row4" class="row_heading level0 row4" >4</th>
      <td id="T_b8361_row4_col0" class="data row4 col0" >2021-12-29</td>
      <td id="T_b8361_row4_col1" class="data row4 col1" >1</td>
    </tr>
  </tbody>
</table>



First, we take a look at our data.

```python
fig, ax = plt.subplots(figsize=(16, 11))
ax.plot(df['Frequency'])
ax.set_xlabel('Time (Days)')
ax.set_ylabel('Frequency of Tweets')
fig.autofmt_xdate()
plt.tight_layout()
```

We then create the actual value we're trying to predict as "Next Day Frequency".

```python
df['Next Day Frequency'] = df['Frequency'].shift(-1)
```

Now, we split our data set into training and testing data. In this case, we try and use the data points prior to the start of the campaign period to predict the behavior of daily frequency throughout the time we cover.

```python
train = df[:45]
test = df[45:]
test = test.drop(test.tail(1).index) # Drop last row
train.head().style.set_properties(**{'text-align': 'left'})
```



<style type="text/css">
#T_86ecf_row0_col0, #T_86ecf_row0_col1, #T_86ecf_row0_col2, #T_86ecf_row1_col0, #T_86ecf_row1_col1, #T_86ecf_row1_col2, #T_86ecf_row2_col0, #T_86ecf_row2_col1, #T_86ecf_row2_col2, #T_86ecf_row3_col0, #T_86ecf_row3_col1, #T_86ecf_row3_col2, #T_86ecf_row4_col0, #T_86ecf_row4_col1, #T_86ecf_row4_col2 {
  text-align: left;
}
</style>
<table id="T_86ecf">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_86ecf_level0_col0" class="col_heading level0 col0" >Date</th>
      <th id="T_86ecf_level0_col1" class="col_heading level0 col1" >Frequency</th>
      <th id="T_86ecf_level0_col2" class="col_heading level0 col2" >Next Day Frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_86ecf_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_86ecf_row0_col0" class="data row0 col0" >2021-12-25</td>
      <td id="T_86ecf_row0_col1" class="data row0 col1" >0</td>
      <td id="T_86ecf_row0_col2" class="data row0 col2" >1.000000</td>
    </tr>
    <tr>
      <th id="T_86ecf_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_86ecf_row1_col0" class="data row1 col0" >2021-12-26</td>
      <td id="T_86ecf_row1_col1" class="data row1 col1" >1</td>
      <td id="T_86ecf_row1_col2" class="data row1 col2" >0.000000</td>
    </tr>
    <tr>
      <th id="T_86ecf_level0_row2" class="row_heading level0 row2" >2</th>
      <td id="T_86ecf_row2_col0" class="data row2 col0" >2021-12-27</td>
      <td id="T_86ecf_row2_col1" class="data row2 col1" >0</td>
      <td id="T_86ecf_row2_col2" class="data row2 col2" >1.000000</td>
    </tr>
    <tr>
      <th id="T_86ecf_level0_row3" class="row_heading level0 row3" >3</th>
      <td id="T_86ecf_row3_col0" class="data row3 col0" >2021-12-28</td>
      <td id="T_86ecf_row3_col1" class="data row3 col1" >1</td>
      <td id="T_86ecf_row3_col2" class="data row3 col2" >1.000000</td>
    </tr>
    <tr>
      <th id="T_86ecf_level0_row4" class="row_heading level0 row4" >4</th>
      <td id="T_86ecf_row4_col0" class="data row4 col0" >2021-12-29</td>
      <td id="T_86ecf_row4_col1" class="data row4 col1" >1</td>
      <td id="T_86ecf_row4_col2" class="data row4 col2" >0.000000</td>
    </tr>
  </tbody>
</table>



We create a baseline predictor that essentially just copies the frequency for ease of comparison later.

```python
test = test.copy()
test['Baseline Prediction'] = test['Frequency']

test.head().style.set_properties(**{'text-align': 'left'})
```



<style type="text/css">
#T_f41c1_row0_col0, #T_f41c1_row0_col1, #T_f41c1_row0_col2, #T_f41c1_row0_col3, #T_f41c1_row1_col0, #T_f41c1_row1_col1, #T_f41c1_row1_col2, #T_f41c1_row1_col3, #T_f41c1_row2_col0, #T_f41c1_row2_col1, #T_f41c1_row2_col2, #T_f41c1_row2_col3, #T_f41c1_row3_col0, #T_f41c1_row3_col1, #T_f41c1_row3_col2, #T_f41c1_row3_col3, #T_f41c1_row4_col0, #T_f41c1_row4_col1, #T_f41c1_row4_col2, #T_f41c1_row4_col3 {
  text-align: left;
}
</style>
<table id="T_f41c1">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_f41c1_level0_col0" class="col_heading level0 col0" >Date</th>
      <th id="T_f41c1_level0_col1" class="col_heading level0 col1" >Frequency</th>
      <th id="T_f41c1_level0_col2" class="col_heading level0 col2" >Next Day Frequency</th>
      <th id="T_f41c1_level0_col3" class="col_heading level0 col3" >Baseline Prediction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_f41c1_level0_row0" class="row_heading level0 row0" >45</th>
      <td id="T_f41c1_row0_col0" class="data row0 col0" >2022-02-08</td>
      <td id="T_f41c1_row0_col1" class="data row0 col1" >1</td>
      <td id="T_f41c1_row0_col2" class="data row0 col2" >6.000000</td>
      <td id="T_f41c1_row0_col3" class="data row0 col3" >1</td>
    </tr>
    <tr>
      <th id="T_f41c1_level0_row1" class="row_heading level0 row1" >46</th>
      <td id="T_f41c1_row1_col0" class="data row1 col0" >2022-02-09</td>
      <td id="T_f41c1_row1_col1" class="data row1 col1" >6</td>
      <td id="T_f41c1_row1_col2" class="data row1 col2" >7.000000</td>
      <td id="T_f41c1_row1_col3" class="data row1 col3" >6</td>
    </tr>
    <tr>
      <th id="T_f41c1_level0_row2" class="row_heading level0 row2" >47</th>
      <td id="T_f41c1_row2_col0" class="data row2 col0" >2022-02-10</td>
      <td id="T_f41c1_row2_col1" class="data row2 col1" >7</td>
      <td id="T_f41c1_row2_col2" class="data row2 col2" >0.000000</td>
      <td id="T_f41c1_row2_col3" class="data row2 col3" >7</td>
    </tr>
    <tr>
      <th id="T_f41c1_level0_row3" class="row_heading level0 row3" >48</th>
      <td id="T_f41c1_row3_col0" class="data row3 col0" >2022-02-11</td>
      <td id="T_f41c1_row3_col1" class="data row3 col1" >0</td>
      <td id="T_f41c1_row3_col2" class="data row3 col2" >0.000000</td>
      <td id="T_f41c1_row3_col3" class="data row3 col3" >0</td>
    </tr>
    <tr>
      <th id="T_f41c1_level0_row4" class="row_heading level0 row4" >49</th>
      <td id="T_f41c1_row4_col0" class="data row4 col0" >2022-02-12</td>
      <td id="T_f41c1_row4_col1" class="data row4 col1" >0</td>
      <td id="T_f41c1_row4_col2" class="data row4 col2" >5.000000</td>
      <td id="T_f41c1_row4_col3" class="data row4 col3" >0</td>
    </tr>
  </tbody>
</table>



### Decision Tree

This model uses a simple decision tree with no cross-validation or hyperparameter tuning to perform regression on the training set and continuing on to predict throughout the test data set.

```python
from sklearn.tree import DecisionTreeRegressor
X_train = train['Frequency'].values.reshape(-1,1)
y_train = train['Next Day Frequency'].values.reshape(-1,1)
X_test = test['Frequency'].values.reshape(-1,1)

# Initialize the model
dt_reg = DecisionTreeRegressor(random_state=42)

# Fit the model
dt_reg.fit(X=X_train, y=y_train)

# Make predictions
dt_pred = dt_reg.predict(X_test)

# Assign predictions to a new column in test
test['Decision Tree Prediction'] = dt_pred

test.head().style.set_properties(**{'text-align': 'left'})
```



<style type="text/css">
#T_e1e5b_row0_col0, #T_e1e5b_row0_col1, #T_e1e5b_row0_col2, #T_e1e5b_row0_col3, #T_e1e5b_row0_col4, #T_e1e5b_row1_col0, #T_e1e5b_row1_col1, #T_e1e5b_row1_col2, #T_e1e5b_row1_col3, #T_e1e5b_row1_col4, #T_e1e5b_row2_col0, #T_e1e5b_row2_col1, #T_e1e5b_row2_col2, #T_e1e5b_row2_col3, #T_e1e5b_row2_col4, #T_e1e5b_row3_col0, #T_e1e5b_row3_col1, #T_e1e5b_row3_col2, #T_e1e5b_row3_col3, #T_e1e5b_row3_col4, #T_e1e5b_row4_col0, #T_e1e5b_row4_col1, #T_e1e5b_row4_col2, #T_e1e5b_row4_col3, #T_e1e5b_row4_col4 {
  text-align: left;
}
</style>
<table id="T_e1e5b">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_e1e5b_level0_col0" class="col_heading level0 col0" >Date</th>
      <th id="T_e1e5b_level0_col1" class="col_heading level0 col1" >Frequency</th>
      <th id="T_e1e5b_level0_col2" class="col_heading level0 col2" >Next Day Frequency</th>
      <th id="T_e1e5b_level0_col3" class="col_heading level0 col3" >Baseline Prediction</th>
      <th id="T_e1e5b_level0_col4" class="col_heading level0 col4" >Decision Tree Prediction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_e1e5b_level0_row0" class="row_heading level0 row0" >45</th>
      <td id="T_e1e5b_row0_col0" class="data row0 col0" >2022-02-08</td>
      <td id="T_e1e5b_row0_col1" class="data row0 col1" >1</td>
      <td id="T_e1e5b_row0_col2" class="data row0 col2" >6.000000</td>
      <td id="T_e1e5b_row0_col3" class="data row0 col3" >1</td>
      <td id="T_e1e5b_row0_col4" class="data row0 col4" >1.400000</td>
    </tr>
    <tr>
      <th id="T_e1e5b_level0_row1" class="row_heading level0 row1" >46</th>
      <td id="T_e1e5b_row1_col0" class="data row1 col0" >2022-02-09</td>
      <td id="T_e1e5b_row1_col1" class="data row1 col1" >6</td>
      <td id="T_e1e5b_row1_col2" class="data row1 col2" >7.000000</td>
      <td id="T_e1e5b_row1_col3" class="data row1 col3" >6</td>
      <td id="T_e1e5b_row1_col4" class="data row1 col4" >2.000000</td>
    </tr>
    <tr>
      <th id="T_e1e5b_level0_row2" class="row_heading level0 row2" >47</th>
      <td id="T_e1e5b_row2_col0" class="data row2 col0" >2022-02-10</td>
      <td id="T_e1e5b_row2_col1" class="data row2 col1" >7</td>
      <td id="T_e1e5b_row2_col2" class="data row2 col2" >0.000000</td>
      <td id="T_e1e5b_row2_col3" class="data row2 col3" >7</td>
      <td id="T_e1e5b_row2_col4" class="data row2 col4" >4.000000</td>
    </tr>
    <tr>
      <th id="T_e1e5b_level0_row3" class="row_heading level0 row3" >48</th>
      <td id="T_e1e5b_row3_col0" class="data row3 col0" >2022-02-11</td>
      <td id="T_e1e5b_row3_col1" class="data row3 col1" >0</td>
      <td id="T_e1e5b_row3_col2" class="data row3 col2" >0.000000</td>
      <td id="T_e1e5b_row3_col3" class="data row3 col3" >0</td>
      <td id="T_e1e5b_row3_col4" class="data row3 col4" >1.055556</td>
    </tr>
    <tr>
      <th id="T_e1e5b_level0_row4" class="row_heading level0 row4" >49</th>
      <td id="T_e1e5b_row4_col0" class="data row4 col0" >2022-02-12</td>
      <td id="T_e1e5b_row4_col1" class="data row4 col1" >0</td>
      <td id="T_e1e5b_row4_col2" class="data row4 col2" >5.000000</td>
      <td id="T_e1e5b_row4_col3" class="data row4 col3" >0</td>
      <td id="T_e1e5b_row4_col4" class="data row4 col4" >1.055556</td>
    </tr>
  </tbody>
</table>



### Gradient Boosting

We now try a similar model frim `scikit-learn` so we can compare the results of the two.

```python
from sklearn.ensemble import GradientBoostingRegressor
gbr = GradientBoostingRegressor(random_state=42)
gbr.fit(X_train, y=y_train.ravel())
gbr_pred = gbr.predict(X_test)
test['Gradient Boosting Prediction'] = gbr_pred

test.head().style.set_properties(**{'text-align': 'left'})
```



<style type="text/css">
#T_99406_row0_col0, #T_99406_row0_col1, #T_99406_row0_col2, #T_99406_row0_col3, #T_99406_row0_col4, #T_99406_row0_col5, #T_99406_row1_col0, #T_99406_row1_col1, #T_99406_row1_col2, #T_99406_row1_col3, #T_99406_row1_col4, #T_99406_row1_col5, #T_99406_row2_col0, #T_99406_row2_col1, #T_99406_row2_col2, #T_99406_row2_col3, #T_99406_row2_col4, #T_99406_row2_col5, #T_99406_row3_col0, #T_99406_row3_col1, #T_99406_row3_col2, #T_99406_row3_col3, #T_99406_row3_col4, #T_99406_row3_col5, #T_99406_row4_col0, #T_99406_row4_col1, #T_99406_row4_col2, #T_99406_row4_col3, #T_99406_row4_col4, #T_99406_row4_col5 {
  text-align: left;
}
</style>
<table id="T_99406">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_99406_level0_col0" class="col_heading level0 col0" >Date</th>
      <th id="T_99406_level0_col1" class="col_heading level0 col1" >Frequency</th>
      <th id="T_99406_level0_col2" class="col_heading level0 col2" >Next Day Frequency</th>
      <th id="T_99406_level0_col3" class="col_heading level0 col3" >Baseline Prediction</th>
      <th id="T_99406_level0_col4" class="col_heading level0 col4" >Decision Tree Prediction</th>
      <th id="T_99406_level0_col5" class="col_heading level0 col5" >Gradient Boosting Prediction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_99406_level0_row0" class="row_heading level0 row0" >45</th>
      <td id="T_99406_row0_col0" class="data row0 col0" >2022-02-08</td>
      <td id="T_99406_row0_col1" class="data row0 col1" >1</td>
      <td id="T_99406_row0_col2" class="data row0 col2" >6.000000</td>
      <td id="T_99406_row0_col3" class="data row0 col3" >1</td>
      <td id="T_99406_row0_col4" class="data row0 col4" >1.400000</td>
      <td id="T_99406_row0_col5" class="data row0 col5" >1.400002</td>
    </tr>
    <tr>
      <th id="T_99406_level0_row1" class="row_heading level0 row1" >46</th>
      <td id="T_99406_row1_col0" class="data row1 col0" >2022-02-09</td>
      <td id="T_99406_row1_col1" class="data row1 col1" >6</td>
      <td id="T_99406_row1_col2" class="data row1 col2" >7.000000</td>
      <td id="T_99406_row1_col3" class="data row1 col3" >6</td>
      <td id="T_99406_row1_col4" class="data row1 col4" >2.000000</td>
      <td id="T_99406_row1_col5" class="data row1 col5" >1.999986</td>
    </tr>
    <tr>
      <th id="T_99406_level0_row2" class="row_heading level0 row2" >47</th>
      <td id="T_99406_row2_col0" class="data row2 col0" >2022-02-10</td>
      <td id="T_99406_row2_col1" class="data row2 col1" >7</td>
      <td id="T_99406_row2_col2" class="data row2 col2" >0.000000</td>
      <td id="T_99406_row2_col3" class="data row2 col3" >7</td>
      <td id="T_99406_row2_col4" class="data row2 col4" >4.000000</td>
      <td id="T_99406_row2_col5" class="data row2 col5" >3.999933</td>
    </tr>
    <tr>
      <th id="T_99406_level0_row3" class="row_heading level0 row3" >48</th>
      <td id="T_99406_row3_col0" class="data row3 col0" >2022-02-11</td>
      <td id="T_99406_row3_col1" class="data row3 col1" >0</td>
      <td id="T_99406_row3_col2" class="data row3 col2" >0.000000</td>
      <td id="T_99406_row3_col3" class="data row3 col3" >0</td>
      <td id="T_99406_row3_col4" class="data row3 col4" >1.055556</td>
      <td id="T_99406_row3_col5" class="data row3 col5" >1.055567</td>
    </tr>
    <tr>
      <th id="T_99406_level0_row4" class="row_heading level0 row4" >49</th>
      <td id="T_99406_row4_col0" class="data row4 col0" >2022-02-12</td>
      <td id="T_99406_row4_col1" class="data row4 col1" >0</td>
      <td id="T_99406_row4_col2" class="data row4 col2" >5.000000</td>
      <td id="T_99406_row4_col3" class="data row4 col3" >0</td>
      <td id="T_99406_row4_col4" class="data row4 col4" >1.055556</td>
      <td id="T_99406_row4_col5" class="data row4 col5" >1.055567</td>
    </tr>
  </tbody>
</table>



## Evaluation

We use the Mean Absolute Percentage Error to interpret how far/close our predictions are from the actual values and compare this across the models we've created.

```python
def mape(y_true, y_pred):
    return round(np.mean(np.abs((y_true - y_pred) / (y_true + 1))) * 100, 2) # +1 to avoid division by zero

baseline_mape = mape(test['Next Day Frequency'], test['Baseline Prediction'])
dt_mape = mape(test['Next Day Frequency'], test['Decision Tree Prediction'])
gbr_mape = mape(test['Next Day Frequency'], test['Gradient Boosting Prediction'])
# Generate bar plot
fig, ax = plt.subplots(figsize=(7, 5))
x = ['Baseline', 'Decision Tree', 'Gradient Boosting']
y = [baseline_mape, dt_mape, gbr_mape]
ax.bar(x, y, width=0.4)
ax.set_xlabel('Regressor models')
ax.set_ylabel('MAPE (%)')
ax.set_ylim(0, 100)

plt.show()
```

![png]({{ site.baseurl }}/public/images/results/results_33_0.png){: .center-image }

![png]({{ site.baseurl }}/public/images/results/results_33_1.png){: .center-image }

![png]({{ site.baseurl }}/public/images/results/results_33_2.png){: .center-image }

![png]({{ site.baseurl }}/public/images/results/results_33_3.png){: .center-image }

## Visualizing its behavior

Now that we have our predictions, we can look at the behavior of the models compared to the actual value.

```python
df_combined = pd.concat([train, test], axis=0)
```

```python
fig, ax = plt.subplots(figsize=(16, 11))
ax.plot(df_combined['Frequency'], marker='.', color='#1D2B42', label='Actual')
# ax.plot(df_combined['Baseline Prediction'], linestyle=':', color='gray', label='Baseline')
ax.plot(df_combined['Decision Tree Prediction'], linestyle='-', color='#22A4F2', label='Predicted')
# ax.plot(df_combined['Gradient Boosting Prediction'], linestyle=(0, (3, 10, 1, 10)), color='blue', label='Gradient Boosting')

plt.vlines(x = [45, 133], ymin = 0, ymax = max(ax.get_ylim()),
           colors = '#FFDA21')

plt.vlines(x = 135, ymin = 0, ymax = max(ax.get_ylim()),
           colors = '#E9012B')

ax.set_xlabel('Timesteps')
ax.set_ylabel('Frequency')

plt.suptitle('Tweet Frequency-per-Day')
plt.legend(loc=1)
fig.autofmt_xdate()
plt.tight_layout()
```

Now we compare the behavior of the cumulative sums to help us look at the predicted cumulative frequency of tweets for each timestep.

```python
df_combined['Sum'] = df_combined['Frequency'].cumsum()
df_combined['BP Sum'] = df_combined['Baseline Prediction'].cumsum() + df_combined['Sum'][45]
df_combined['DT Sum'] = df_combined['Decision Tree Prediction'].cumsum() + df_combined['Sum'][45]
df_combined['GB Sum'] = df_combined['Gradient Boosting Prediction'].cumsum() + df_combined['Sum'][45]
```

```python
fig, ax = plt.subplots(figsize=(16, 11))
ax.plot(df_combined['Sum'], linestyle='-', color='#1D2B42', label='Actual', linewidth=10)
# ax.plot(df_combined['BP Sum'], marker='o', color='gray', label='Baseline')
ax.plot(df_combined['DT Sum'], linestyle='-', color='#22A4F2', label='Predicted', linewidth=10)
# ax.plot(df_combined['GB Sum'], linestyle=':', color='red', label='Gradient Boosting')
ax.set_xlabel('Timesteps')
ax.set_ylabel('Frequency')

plt.vlines(x = [45, 133], ymin = 0, ymax = max(ax.get_ylim()),
           colors = '#FFDA21',
           linewidth=5)

plt.vlines(x = 135, ymin = 0, ymax = max(ax.get_ylim()),
           colors = '#E9012B',
           linewidth=5)

plt.suptitle('Cumulative Sum of Tweet Frequency-per-Day')
plt.legend(loc=2)
fig.autofmt_xdate()
plt.tight_layout()
```

As we can see, the actual value is much higher than any of our models' predictions but exploring other ways (different timesteps, training on cumulative sums, etc.) could lead to better performance. 
